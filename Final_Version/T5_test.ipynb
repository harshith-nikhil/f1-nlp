{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8bc7c9f-f43c-4c50-b049-8ff9dc66b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a6d140-2b70-477b-85f8-4430b54e96b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"./flan_t5_f1_qa\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_dir)\n",
    "model     = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "# Build a simple generation pipeline\n",
    "qa_gen = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0  # change to -1 if no GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0226ae7-ccde-49cb-b8bd-4d6bd90b5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"f1_test_new.json\",\n",
    "    field=\"data\"\n",
    ")[\"train\"]\n",
    "\n",
    "test_rows = []\n",
    "for item in raw_test:\n",
    "    for para in item[\"paragraphs\"]:\n",
    "        ctx = para[\"context\"]\n",
    "        for qa in para[\"qas\"]:\n",
    "            # skip impossible if you have them\n",
    "            if not qa[\"answers\"]:\n",
    "                continue\n",
    "            test_rows.append({\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"context\":  ctx,\n",
    "                \"answer\":   qa[\"answers\"][0][\"text\"]\n",
    "            })\n",
    "test_ds = Dataset.from_list(test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2891a2-ca46-48ff-aeb8-fa3434f8c2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds, refs = [], []\n",
    "for ex in test_ds:\n",
    "    prompt = f\"question: {ex['question']}  context: {ex['context']}\"\n",
    "    out = qa_gen(prompt, max_length=32, do_sample=False)[0][\"generated_text\"].strip()\n",
    "    preds.append(out)\n",
    "    refs.append(ex[\"answer\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd6b910-dff2-4b76-8e44-ac981e7678cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = sum(p == r for p, r in zip(preds, refs)) / len(refs)\n",
    "\n",
    "# Load text‐similarity metrics\n",
    "bleu  = evaluate.load(\"bleu\").compute(predictions=preds, references=[[r] for r in refs])[\"bleu\"]\n",
    "rouge = evaluate.load(\"rouge\").compute(predictions=preds, references=refs, use_stemmer=True)[\"rougeL\"]\n",
    "chrf  = evaluate.load(\"chrf\").compute(predictions=preds, references=refs)[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dd745a3-0a27-4140-97d2-1cc246f2a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set performance:\n",
      " • exact_match : 0.468\n",
      " • bleu        : 0.000\n",
      " • rougeL      : 0.468\n",
      " • chrf        : 21.818\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"exact_match\": em,\n",
    "    \"bleu\":        bleu,\n",
    "    \"rougeL\":      rouge,\n",
    "    \"chrf\":        chrf\n",
    "}\n",
    "print(\"Test set performance:\")\n",
    "for k, v in results.items():\n",
    "    print(f\" • {k:12s}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb04f126-f1e9-4a1a-b77b-332138b30440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions:\n",
      "Q: Who won the Italian Grand Prix?\n",
      "GT: 'Max Verstappen'\n",
      "PR: 'Max Verstappen'\n",
      "---\n",
      "Q: Which driver finished in position 1?\n",
      "GT: 'Max Verstappen'\n",
      "PR: 'Sergio Perez'\n",
      "---\n",
      "Q: Which driver finished in position 2?\n",
      "GT: 'Sergio Perez'\n",
      "PR: 'Max Verstappen'\n",
      "---\n",
      "Q: Which driver finished in position 3?\n",
      "GT: 'Carlos Sainz'\n",
      "PR: 'Charles Leclerc'\n",
      "---\n",
      "Q: Which driver finished in position 4?\n",
      "GT: 'Charles Leclerc'\n",
      "PR: 'George Russell'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Q: {test_ds[i]['question']}\")\n",
    "    print(f\"GT: {refs[i]!r}\")\n",
    "    print(f\"PR: {preds[i]!r}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea21ef-23a0-4f9d-9c71-8df5ba6f0764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
