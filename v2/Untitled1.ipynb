{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb7277a-bcc3-4b9f-8fdc-add107fdcea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccdb8c2-d655-48fd-9542-80c0e9b6018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779e1b77-3264-4f68-bf37-c5513b570236",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"./qa_roberta_checkpoint\"    # ‚Üê adjust this to your saved model folder\n",
    "\n",
    "# 2) Re‚Äëload tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "model     = AutoModelForQuestionAnswering.from_pretrained(CHECKPOINT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc95b32-751e-4498-a09e-bb84eaa764bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "qa = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0    # set to -1 if you want CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce622624-9768-4a3f-a8ff-40e3051966e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5bbe2e02ee4ba79151aa38c6f1161d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ca050f94904de0bd9756361116307e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59fa7cbf096493f9a41e1516d56b880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = evaluate.load(\"squad\")\n",
    "raw = load_dataset(\"json\", data_files={\"validation\":\"val.json\"}, field=\"data\")[\n",
    "    \"validation\"\n",
    "][0][\"paragraphs\"][0]\n",
    "context = raw[\"context\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b48d8fe-5148-4b46-89ca-252a1418f9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "preds, refs = [], []\n",
    "for qa_pair in raw[\"qas\"]:\n",
    "    qid   = qa_pair[\"id\"]\n",
    "    qtext = qa_pair[\"question\"]\n",
    "    gold  = qa_pair[\"answers\"][0][\"text\"]\n",
    "    out   = qa(question=qtext, context=context)\n",
    "\n",
    "    # collect for SQuAD metric\n",
    "    preds.append({\"id\": qid, \"prediction_text\": out[\"answer\"]})\n",
    "    refs.append({ \"id\": qid,\n",
    "                  \"answers\": [{\"text\": gold,\n",
    "                               \"answer_start\": context.find(gold)}] })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb641837-797c-44c1-b3c0-583f1cb1a5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Exact Match: 0.00\n",
      "‚Üí F1 Score:    0.00\n"
     ]
    }
   ],
   "source": [
    "metrics = squad.compute(predictions=preds, references=refs)\n",
    "print(\"‚Üí Exact Match: {:.2f}\".format(metrics[\"exact_match\"]))\n",
    "print(\"‚Üí F1 Score:    {:.2f}\".format(metrics[\"f1\"]))\n",
    "\n",
    "# 5) Or simply do a quick human‚Äëin‚Äëthe‚Äëloop test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3ab303-d80a-403d-8cdf-60a0cceba9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Manual Tests ===\n",
      "Q: Who won the race?\n",
      "A: ).  (score 0.000)\n",
      "\n",
      "Q: How many points did Lewis Hamilton score?\n",
      "A: ).  (score 0.000)\n",
      "\n",
      "Q: What was the fastest lap overall?\n",
      "A: ).  (score 0.000)\n",
      "\n",
      "Q: Which driver retired?\n",
      "A: ).  (score 0.000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Manual Tests ===\")\n",
    "questions = [\n",
    "    \"Who won the race?\",\n",
    "    \"How many points did Lewis Hamilton score?\",\n",
    "    \"What was the fastest lap overall?\",\n",
    "    \"Which driver retired?\"\n",
    "]\n",
    "for q in questions:\n",
    "    ans = qa(question=q, context=context)\n",
    "    print(f\"Q: {q}\\nA: {ans['answer']}  (score {ans['score']:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aec9d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: q19_driverLogan Sargeant_CompoundMEDIUM\n",
      "  Predicted: ').'\n",
      "  Gold     : '18'\n",
      "  answer_start: 383\n",
      "---\n",
      "ID: q3_driverFernando Alonso\n",
      "  Predicted: ').'\n",
      "  Gold     : '2'\n",
      "  answer_start: 127\n",
      "---\n",
      "ID: q20_driverGeorge Russell_CompoundSOFT\n",
      "  Predicted: '.'\n",
      "  Gold     : 'nan'\n",
      "  answer_start: 330\n",
      "---\n",
      "ID: q20_driverGuanyu Zhou_CompoundSOFT\n",
      "  Predicted: ').'\n",
      "  Gold     : 'nan'\n",
      "  answer_start: 330\n",
      "---\n",
      "ID: q3_driverSergio Perez\n",
      "  Predicted: ').'\n",
      "  Gold     : '16'\n",
      "  answer_start: 1579\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# After building `predictions` and `references` lists:\n",
    "\n",
    "for i in range(5):\n",
    "    p = preds[i]\n",
    "    r = refs[i]\n",
    "    print(f\"ID: {p['id']}\")\n",
    "    print(\"  Predicted:\", repr(p[\"prediction_text\"]))\n",
    "    print(\"  Gold     :\", repr(r[\"answers\"][0][\"text\"]))\n",
    "    print(\"  answer_start:\", r[\"answers\"][0].get(\"answer_start\"))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d73e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Validation examples: 72\n",
      "\n",
      "ID: q19_driverKevin Magnussen_CompoundMEDIUM\n",
      " Predicted: '.'\n",
      " Gold     : '0'\n",
      " Start    : 89\n",
      "---\n",
      "ID: q20_driverLewis Hamilton_CompoundINTERMEDIATE\n",
      " Predicted: ').'\n",
      " Gold     : '01:24.651'\n",
      " Start    : 5841\n",
      "---\n",
      "ID: q20_driverCarlos Sainz_CompoundHARD\n",
      " Predicted: ').'\n",
      " Gold     : '01:17.171'\n",
      " Start    : 6021\n",
      "---\n",
      "ID: q3_driverCharles Leclerc\n",
      " Predicted: '.'\n",
      " Gold     : '6'\n",
      " Start    : 704\n",
      "---\n",
      "ID: q20_driverPierre Gasly_CompoundHARD\n",
      " Predicted: ').'\n",
      " Gold     : '01:16.839'\n",
      " Start    : 2238\n",
      "---\n",
      "\n",
      "‚úÖ Exact Match: 0.00%\n",
      "‚úÖ F1 Score:    0.00%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1b4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b81c520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36248f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö°Ô∏è F1 QA Interactive Demo\n",
      "Type any question about the race (or ‚Äòexit‚Äô to quit).\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d2c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# 1) Point to your saved checkpoint folder (no leading ‚Äú./‚Äù)\n",
    "CHECKPOINT = \"qa_roberta_checkpoint\"\n",
    "\n",
    "# 2) Reload the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT, local_files_only=True)\n",
    "model     = AutoModelForQuestionAnswering.from_pretrained(CHECKPOINT, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749db375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 3) Build the HF QA pipeline\n",
    "qa = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0   # set to -1 if you only have CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8a3ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö°Ô∏è F1 QA Interactive Demo\n",
      "Type any question about the race (or ‚Äòexit‚Äô to quit).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Read and extract the exact context you trained on\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"full\": \"f1_gp_qa.json\"},\n",
    "    field=\"data\"\n",
    ")[\"full\"]\n",
    "# assume first (and only) race, first paragraph\n",
    "context = raw[0][\"paragraphs\"][0][\"context\"]\n",
    "\n",
    "print(\"\\n‚ö°Ô∏è F1 QA Interactive Demo\")\n",
    "print(\"Type any question about the race (or ‚Äòexit‚Äô to quit).\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a503f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Interactive loop\n",
    "while True:\n",
    "    question = input(\"Q: \").strip()\n",
    "    if not question or question.lower() in (\"exit\", \"quit\"):\n",
    "        print(\"üëã Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # chunk & stride exactly as you did during training\n",
    "    result = qa(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        max_length=384,\n",
    "        doc_stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        top_k=1,\n",
    "    )\n",
    "\n",
    "    print(f\"A: {result['answer']}   (confidence {result['score']:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c5d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö°Ô∏è F1 QA Interactive (type ‚Äòexit‚Äô to quit)\n",
      "\n",
      "A: ).   (confidence 0.00)\n",
      "\n",
      "A: ).   (confidence 0.00)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`question` cannot be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 5) Call the pipeline with chunking parameters\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# same as training\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# same as training\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# return the single best span\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   (confidence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:395\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[1;32m    390\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    393\u001b[0m     )\n\u001b[0;32m--> 395\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:227\u001b[0m, in \u001b[0;36mQuestionAnsweringArgumentHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs):\n\u001b[0;32m--> 227\u001b[0m     inputs[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:169\u001b[0m, in \u001b[0;36mQuestionAnsweringArgumentHandler.normalize\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item[k], \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item[k]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QuestionAnsweringPipeline\u001b[38;5;241m.\u001b[39mcreate_sample(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mitem)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m argument needs to be of type (SquadExample, dict)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: `question` cannot be empty"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9c885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
